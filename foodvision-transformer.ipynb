{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":33884,"sourceType":"datasetVersion","datasetId":1864}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\n# ============================================================================\n# Install Required Packages\n# ============================================================================\n!pip install -q transformers datasets accelerate evaluate\n\n# ============================================================================\n# Import Libraries\n# ============================================================================\nimport os\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom PIL import Image\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom torchvision.transforms import functional as F\n\nfrom transformers import (\n    ViTImageProcessor, \n    ViTForImageClassification,\n    TrainingArguments,\n    Trainer\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n\n# ============================================================================\n# CELL 3: Configuration\n# ============================================================================\nclass Config:\n    DATA_DIR = '/kaggle/input/food41/images'\n    OUTPUT_DIR = '/kaggle/working/vit_food41_model'\n    \n    MODEL_NAME = 'google/vit-base-patch16-224'\n    NUM_LABELS = 101   # ✅ Fix here\n    \n    BATCH_SIZE = 32\n    NUM_EPOCHS = 5\n    LEARNING_RATE = 2e-5\n    WARMUP_STEPS = 500\n    WEIGHT_DECAY = 0.01\n    \n    FP16 = True\n    GRADIENT_ACCUMULATION_STEPS = 2\n    \n    VAL_SIZE = 0.1\n    TEST_SIZE = 0.1\n    \n    SEED = 42\n    NUM_WORKERS = 2\n\n\nconfig = Config()\n\n# Set seeds\ntorch.manual_seed(config.SEED)\nnp.random.seed(config.SEED)\n\n# ============================================================================\n# Data Exploration\n# ============================================================================\nprint(\"Exploring dataset...\")\nprint(f\"Dataset location: {config.DATA_DIR}\")\n\nif not os.path.exists(config.DATA_DIR):\n    print(f\"ERROR: Dataset not found at {config.DATA_DIR}\")\n    print(\"Please add the Food-41 dataset to your Kaggle notebook:\")\n    print(\"1. Click 'Add Data' in the right sidebar\")\n    print(\"2. Search for 'food41'\")\n    print(\"3. Add the dataset\")\nelse:\n    categories = sorted([d for d in os.listdir(config.DATA_DIR) \n                        if os.path.isdir(os.path.join(config.DATA_DIR, d))])\n    \n    print(f\"\\nFound {len(categories)} food categories\")\n    print(f\"Sample categories: {categories[:5]}\")\n    \n    # Count images per category\n    category_counts = {}\n    for cat in categories:\n        cat_path = os.path.join(config.DATA_DIR, cat)\n        count = len([f for f in os.listdir(cat_path) \n                    if f.endswith(('.jpg', '.jpeg', '.png'))])\n        category_counts[cat] = count\n    \n    print(f\"\\nTotal images: {sum(category_counts.values())}\")\n    print(f\"Average per category: {np.mean(list(category_counts.values())):.0f}\")\n    print(f\"Min per category: {min(category_counts.values())}\")\n    print(f\"Max per category: {max(category_counts.values())}\")\n\n# ============================================================================\n# Visualize Dataset Distribution\n# ============================================================================\n# Plot category distribution\nplt.figure(figsize=(20, 6))\nsorted_cats = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)\ncats, counts = zip(*sorted_cats)\n\nplt.bar(range(len(cats)), counts, color='steelblue', alpha=0.7)\nplt.axhline(y=np.mean(counts), color='r', linestyle='--', \n            label=f'Average: {np.mean(counts):.0f}')\nplt.xlabel('Food Category', fontsize=12, fontweight='bold')\nplt.ylabel('Number of Images', fontsize=12, fontweight='bold')\nplt.title('Food-41 Dataset Distribution', fontsize=14, fontweight='bold')\nplt.xticks(range(len(cats)), cats, rotation=90, ha='right')\nplt.legend()\nplt.tight_layout()\nplt.savefig('dataset_distribution.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"Distribution plot saved!\")\n\n# ============================================================================\n# Visualize Sample Images\n# ============================================================================\n# Show sample images from different categories\nfig, axes = plt.subplots(3, 5, figsize=(15, 9))\naxes = axes.flatten()\n\nsample_categories = np.random.choice(categories, size=min(15, len(categories)), \n                                    replace=False)\n\nfor idx, cat in enumerate(sample_categories):\n    cat_path = os.path.join(config.DATA_DIR, cat)\n    images = [f for f in os.listdir(cat_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n    \n    if images:\n        img_file = np.random.choice(images)\n        img_path = os.path.join(cat_path, img_file)\n        \n        try:\n            img = Image.open(img_path).convert('RGB')\n            axes[idx].imshow(img)\n            axes[idx].set_title(cat.replace('_', ' ').title(), \n                               fontsize=9, fontweight='bold')\n            axes[idx].axis('off')\n        except:\n            axes[idx].axis('off')\n\nplt.tight_layout()\nplt.savefig('sample_images.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# ============================================================================\n# Prepare Data Splits\n# ============================================================================\nprint(\"\\nPreparing train/val/test splits...\")\n\n# Collect all image paths and labels\nall_images = []\nall_labels = []\nlabel_to_idx = {}\n\nfor idx, category in enumerate(categories):\n    label_to_idx[category] = idx\n    category_path = os.path.join(config.DATA_DIR, category)\n    \n    for img_file in os.listdir(category_path):\n        if img_file.endswith(('.jpg', '.jpeg', '.png')):\n            img_path = os.path.join(category_path, img_file)\n            all_images.append(img_path)\n            all_labels.append(idx)\n\nprint(f\"Total images collected: {len(all_images)}\")\n\n# Create stratified splits\ntrain_val_images, test_images, train_val_labels, test_labels = train_test_split(\n    all_images, all_labels, \n    test_size=config.TEST_SIZE, \n    random_state=config.SEED,\n    stratify=all_labels\n)\n\ntrain_images, val_images, train_labels, val_labels = train_test_split(\n    train_val_images, train_val_labels,\n    test_size=config.VAL_SIZE / (1 - config.TEST_SIZE),\n    random_state=config.SEED,\n    stratify=train_val_labels\n)\n\nprint(f\"Train: {len(train_images)} ({len(train_images)/len(all_images)*100:.1f}%)\")\nprint(f\"Val: {len(val_images)} ({len(val_images)/len(all_images)*100:.1f}%)\")\nprint(f\"Test: {len(test_images)} ({len(test_images)/len(all_images)*100:.1f}%)\")\n\n# Save label mapping\nidx_to_label = {v: k for k, v in label_to_idx.items()}\nwith open('label_mapping.json', 'w') as f:\n    json.dump(idx_to_label, f, indent=2)\n\nprint(\"\\nLabel mapping saved!\")\n\n# ============================================================================\n# Custom Dataset Class\n# ============================================================================\nclass Food41Dataset(Dataset):\n    \"\"\"Custom Dataset for Food-41\"\"\"\n    \n    def __init__(self, image_paths, labels, processor, is_train=False):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.processor = processor\n        self.is_train = is_train\n        \n        # Augmentation transforms\n        if is_train:\n            self.aug_transforms = transforms.Compose([\n                transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n                transforms.RandomHorizontalFlip(p=0.5),\n                transforms.RandomRotation(15),\n                transforms.ColorJitter(\n                    brightness=0.2,\n                    contrast=0.2,\n                    saturation=0.2,\n                    hue=0.1\n                ),\n            ])\n        else:\n            self.aug_transforms = transforms.Compose([\n                transforms.Resize(256),\n                transforms.CenterCrop(224),\n            ])\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # Load image\n        try:\n            image = Image.open(self.image_paths[idx]).convert('RGB')\n        except Exception as e:\n            print(f\"Error loading {self.image_paths[idx]}: {e}\")\n            # Return a blank image if loading fails\n            image = Image.new('RGB', (224, 224), color='white')\n        \n        # Apply augmentation\n        image = self.aug_transforms(image)\n        \n        # Process with ViT processor\n        encoding = self.processor(images=image, return_tensors=\"pt\")\n        \n        # Remove batch dimension\n        pixel_values = encoding['pixel_values'].squeeze(0)\n        \n        return {\n            'pixel_values': pixel_values,\n            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\nprint(\"Dataset class defined!\")\n\n# ============================================================================\n# Load Model and Processor\n# ============================================================================\nprint(\"Loading ViT model and processor...\")\n\nprocessor = ViTImageProcessor.from_pretrained(config.MODEL_NAME)\nmodel = ViTForImageClassification.from_pretrained(\n    config.MODEL_NAME,\n    num_labels=config.NUM_LABELS,\n    ignore_mismatched_sizes=True\n)\n\n# Update model config\nmodel.config.id2label = idx_to_label\nmodel.config.label2id = label_to_idx\n\nprint(f\"Model loaded: {config.MODEL_NAME}\")\nprint(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n\n# ============================================================================\n# Create Datasets\n# ============================================================================\nprint(\"\\nCreating datasets...\")\n\ntrain_dataset = Food41Dataset(train_images, train_labels, processor, is_train=True)\nval_dataset = Food41Dataset(val_images, val_labels, processor, is_train=False)\ntest_dataset = Food41Dataset(test_images, test_labels, processor, is_train=False)\n\nprint(f\"Train dataset: {len(train_dataset)} samples\")\nprint(f\"Val dataset: {len(val_dataset)} samples\")\nprint(f\"Test dataset: {len(test_dataset)} samples\")\n\n# Test dataset loading\nsample = train_dataset[0]\nprint(f\"\\nSample data shape: {sample['pixel_values'].shape}\")\nprint(f\"Sample label: {sample['labels']}\")\n\n# ============================================================================\n# Define Metrics\n# ============================================================================\ndef compute_metrics(eval_pred):\n    \"\"\"Compute accuracy metrics\"\"\"\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    \n    accuracy = accuracy_score(labels, predictions)\n    \n    return {\n        'accuracy': accuracy,\n    }\n\nprint(\"Metrics function defined!\")\n\n# ============================================================================\n# Training Arguments\n# ============================================================================\ntraining_args = TrainingArguments(\n    output_dir=config.OUTPUT_DIR,\n    num_train_epochs=config.NUM_EPOCHS,\n    per_device_train_batch_size=config.BATCH_SIZE,\n    per_device_eval_batch_size=config.BATCH_SIZE,\n    learning_rate=config.LEARNING_RATE,\n    warmup_steps=config.WARMUP_STEPS,\n    weight_decay=config.WEIGHT_DECAY,\n    logging_dir='./logs',\n    logging_steps=50,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model='accuracy',\n    greater_is_better=True,\n    fp16=config.FP16,\n    gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n    report_to=\"none\",\n    save_total_limit=2,\n    dataloader_num_workers=config.NUM_WORKERS,\n)\n\nprint(\"Training arguments configured!\")\nprint(f\"Effective batch size: {config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS}\")\n\n# ============================================================================\n# Initialize Trainer\n# ============================================================================\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\n\nprint(\"Trainer initialized!\")\nprint(\"Ready to start training...\")\n\n# ============================================================================\n# Train Model\n# ============================================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"STARTING TRAINING\")\nprint(\"=\"*60 + \"\\n\")\n\n# Train the model\ntrain_result = trainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING COMPLETE!\")\nprint(\"=\"*60)\nprint(f\"\\nTraining metrics:\")\nprint(f\"  Final loss: {train_result.training_loss:.4f}\")\n\n# ============================================================================\n# Save Model\n# ============================================================================\nprint(\"\\nSaving model...\")\n\n# Save model and processor\ntrainer.save_model(config.OUTPUT_DIR)\nprocessor.save_pretrained(config.OUTPUT_DIR)\n\n# Save label mapping to output dir\nwith open(os.path.join(config.OUTPUT_DIR, 'label_mapping.json'), 'w') as f:\n    json.dump(idx_to_label, f, indent=2)\n\nprint(f\"Model saved to: {config.OUTPUT_DIR}\")\n\n# ============================================================================\n# Evaluate on Validation Set\n# ============================================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"EVALUATING ON VALIDATION SET\")\nprint(\"=\"*60 + \"\\n\")\n\nval_results = trainer.evaluate(val_dataset)\nprint(\"Validation Results:\")\nfor key, value in val_results.items():\n    print(f\"  {key}: {value:.4f}\")\n\n# ============================================================================\n# Evaluate on Test Set\n# ============================================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"EVALUATING ON TEST SET\")\nprint(\"=\"*60 + \"\\n\")\n\n# Get predictions\npredictions = trainer.predict(test_dataset)\npred_labels = np.argmax(predictions.predictions, axis=1)\ntrue_labels = test_labels\n\n# Calculate accuracy\ntest_accuracy = accuracy_score(true_labels, pred_labels)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n\n# Calculate top-k accuracy\ndef top_k_accuracy(y_true, y_pred_probs, k=5):\n    top_k_preds = np.argsort(y_pred_probs, axis=1)[:, -k:]\n    return np.mean([y_true[i] in top_k_preds[i] for i in range(len(y_true))])\n\ntop3_acc = top_k_accuracy(true_labels, predictions.predictions, k=3)\ntop5_acc = top_k_accuracy(true_labels, predictions.predictions, k=5)\n\nprint(f\"Top-3 Accuracy: {top3_acc:.4f}\")\nprint(f\"Top-5 Accuracy: {top5_acc:.4f}\")\n\n# ============================================================================\n# Classification Report\n# ============================================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"DETAILED CLASSIFICATION REPORT\")\nprint(\"=\"*60 + \"\\n\")\n\n# Get category names\ncategory_names = [idx_to_label[i] for i in range(len(categories))]\n\n# Generate classification report\nreport = classification_report(\n    true_labels, \n    pred_labels,\n    target_names=category_names,\n    output_dict=True\n)\n\n# Convert to DataFrame and save\nreport_df = pd.DataFrame(report).transpose()\nreport_df.to_csv('classification_report.csv')\nprint(\"Classification report saved to: classification_report.csv\")\n\n# Display top and bottom performing classes\nreport_df_sorted = report_df.sort_values('f1-score', ascending=False)\nprint(\"\\nTop 10 Best Performing Classes:\")\nprint(report_df_sorted.head(10)[['precision', 'recall', 'f1-score', 'support']])\n\nprint(\"\\nBottom 10 Classes:\")\nprint(report_df_sorted.tail(10)[['precision', 'recall', 'f1-score', 'support']])\n\n# ============================================================================\n# Confusion Matrix\n# ============================================================================\nprint(\"\\nGenerating confusion matrix...\")\n\n# Calculate confusion matrix\ncm = confusion_matrix(true_labels, pred_labels)\n\n# Plot confusion matrix\nplt.figure(figsize=(20, 18))\nsns.heatmap(cm, annot=False, fmt='d', cmap='Blues', \n            xticklabels=category_names, \n            yticklabels=category_names,\n            cbar_kws={'label': 'Count'})\nplt.title('Confusion Matrix - ViT Food-41 Classification', \n          fontsize=16, fontweight='bold', pad=20)\nplt.ylabel('True Label', fontsize=12, fontweight='bold')\nplt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\nplt.xticks(rotation=90, ha='right')\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"Confusion matrix saved!\")\n\n# Analyze major confusions\nprint(\"\\nTop 10 Most Confused Class Pairs:\")\nconfusions = []\nfor i in range(len(cm)):\n    for j in range(len(cm)):\n        if i != j and cm[i][j] > 0:\n            confusions.append((\n                category_names[i], \n                category_names[j], \n                cm[i][j]\n            ))\n\nconfusions.sort(key=lambda x: x[2], reverse=True)\nfor true_class, pred_class, count in confusions[:10]:\n    print(f\"  {true_class} → {pred_class}: {count} times\")\n\n# ============================================================================\n# Sample Predictions\n# ============================================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"SAMPLE PREDICTIONS\")\nprint(\"=\"*60 + \"\\n\")\n\n# Get some test samples\nnum_samples = 6\nsample_indices = np.random.choice(len(test_images), num_samples, replace=False)\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\nfor idx, sample_idx in enumerate(sample_indices):\n    # Load image\n    img_path = test_images[sample_idx]\n    image = Image.open(img_path).convert('RGB')\n    true_label = idx_to_label[test_labels[sample_idx]]\n    \n    # Get prediction\n    inputs = processor(images=image, return_tensors=\"pt\")\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n        probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n        pred_idx = torch.argmax(probs, dim=1).item()\n        confidence = probs[0][pred_idx].item()\n        pred_label = idx_to_label[pred_idx]\n    \n    # Plot\n    axes[idx].imshow(image)\n    color = 'green' if pred_label == true_label else 'red'\n    title = f\"True: {true_label}\\nPred: {pred_label}\\nConf: {confidence:.2%}\"\n    axes[idx].set_title(title, fontsize=10, color=color, fontweight='bold')\n    axes[idx].axis('off')\n\nplt.tight_layout()\nplt.savefig('sample_predictions.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"Sample predictions saved!\")\n\n# ============================================================================\n# Save Results Summary\n# ============================================================================\n# Create results summary\nresults_summary = {\n    'model_name': config.MODEL_NAME,\n    'num_epochs': config.NUM_EPOCHS,\n    'batch_size': config.BATCH_SIZE,\n    'learning_rate': config.LEARNING_RATE,\n    'test_accuracy': float(test_accuracy),\n    'top3_accuracy': float(top3_acc),\n    'top5_accuracy': float(top5_acc),\n    'num_classes': len(categories),\n    'train_samples': len(train_images),\n    'val_samples': len(val_images),\n    'test_samples': len(test_images),\n}\n\nwith open('results_summary.json', 'w') as f:\n    json.dump(results_summary, f, indent=2)\n\nprint(\"\\nResults Summary:\")\nprint(json.dumps(results_summary, indent=2))\nprint(\"\\nResults saved to: results_summary.json\")\n\n# ============================================================================\n# Single Image Prediction Function\n# ============================================================================\ndef predict_single_image(image_path, top_k=5):\n    \"\"\"Predict a single image with top-k results\"\"\"\n    \n    # Load image\n    image = Image.open(image_path).convert('RGB')\n    \n    # Preprocess\n    inputs = processor(images=image, return_tensors=\"pt\")\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    \n    # Predict\n    with torch.no_grad():\n        outputs = model(**inputs)\n        probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n    \n    # Get top-k predictions\n    top_probs, top_indices = torch.topk(probs, k=min(top_k, config.NUM_LABELS))\n    \n    results = []\n    for prob, idx in zip(top_probs[0], top_indices[0]):\n        results.append({\n            'label': idx_to_label[idx.item()],\n            'confidence': prob.item()\n        })\n    \n    return results\n\n# Test the function\nprint(\"\\nTesting prediction function with a random test image...\")\ntest_img_path = test_images[0]\npredictions = predict_single_image(test_img_path)\n\nprint(f\"Image: {test_img_path}\")\nprint(\"Predictions:\")\nfor i, pred in enumerate(predictions, 1):\n    print(f\"  {i}. {pred['label']}: {pred['confidence']:.2%}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ALL TASKS COMPLETE!\")\nprint(\"=\"*60)\nprint(\"\\nGenerated files:\")\nprint(\"  - vit_food41_model/ (trained model)\")\nprint(\"  - label_mapping.json\")\nprint(\"  - classification_report.csv\")\nprint(\"  - confusion_matrix.png\")\nprint(\"  - sample_predictions.png\")\nprint(\"  - dataset_distribution.png\")\nprint(\"  - results_summary.json\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-01T06:49:37.497947Z","iopub.execute_input":"2025-11-01T06:49:37.498298Z","iopub.status.idle":"2025-11-01T09:27:08.306716Z","shell.execute_reply.started":"2025-11-01T06:49:37.498270Z","shell.execute_reply":"2025-11-01T09:27:08.305837Z"}},"outputs":[],"execution_count":null}]}